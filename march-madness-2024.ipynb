{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":70068,"databundleVersionId":7878506,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tcordeu/march-madness-2024?scriptVersionId=167074108\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom scipy.stats import linregress\nfrom tqdm import tqdm\n\nimport glob\nimport lightgbm as lgb\nimport numpy as np\nimport optuna as op\nimport os\nimport pandas as pd\n\nop.logging.set_verbosity(op.logging.WARNING)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-13T19:18:13.82526Z","iopub.execute_input":"2024-03-13T19:18:13.825721Z","iopub.status.idle":"2024-03-13T19:18:19.600594Z","shell.execute_reply.started":"2024-03-13T19:18:13.825679Z","shell.execute_reply":"2024-03-13T19:18:19.599177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/march-machine-learning-mania-2024'","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:18:19.602485Z","iopub.execute_input":"2024-03-13T19:18:19.603051Z","iopub.status.idle":"2024-03-13T19:18:19.607618Z","shell.execute_reply.started":"2024-03-13T19:18:19.60302Z","shell.execute_reply":"2024-03-13T19:18:19.606689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CSV = {}\n\nfor path in glob.glob(DATA_DIR + \"/*.csv\"):\n    CSV[os.path.basename(path).split('.')[0]] = pd.read_csv(path, encoding='cp1252')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:18:19.608988Z","iopub.execute_input":"2024-03-13T19:18:19.609969Z","iopub.status.idle":"2024-03-13T19:18:24.964154Z","shell.execute_reply.started":"2024-03-13T19:18:19.609936Z","shell.execute_reply":"2024-03-13T19:18:24.963128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def device():\n    from tensorflow.python.client import device_lib\n\n    return 'gpu' if len(list(filter(lambda x: x.device_type == 'GPU', device_lib.list_local_devices()))) > 0 else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:18:24.965837Z","iopub.execute_input":"2024-03-13T19:18:24.966128Z","iopub.status.idle":"2024-03-13T19:18:24.971367Z","shell.execute_reply.started":"2024-03-13T19:18:24.966104Z","shell.execute_reply":"2024-03-13T19:18:24.970186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Device: {}\".format(device()))","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:18:24.973171Z","iopub.execute_input":"2024-03-13T19:18:24.973556Z","iopub.status.idle":"2024-03-13T19:18:38.79355Z","shell.execute_reply.started":"2024-03-13T19:18:24.97352Z","shell.execute_reply":"2024-03-13T19:18:38.792356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build DF","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"def build_results(gender):\n    csv_names = ['NCAATourneyCompactResults', 'RegularSeasonCompactResults']\n    csv_names = list(map(lambda x: gender + x, csv_names))\n    csvs      = list(map(lambda x: CSV[x], csv_names))\n    \n    return pd.concat(csvs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_m = build_results('M')\nresults_w = build_results('W')\n\ndisplay(results_m)\ndisplay(results_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_teams(gender):\n    teams = CSV[\"{}Teams\".format(gender)].copy()\n    teams = teams.drop('TeamName', axis=1)\n    teams = teams.set_index('TeamID')\n    \n    return teams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teams_m = build_teams('M')\nteams_w = build_teams('W') # FIXME: Maybe useless since there is no data aside from TeamName.\n\ndisplay(teams_m)\ndisplay(teams_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_elo(teams, data, initial_rating=2000, k=140, alpha=None):\n    '''\n    Calculate Elo ratings for each team based on match data.\n\n    Parameters:\n    - teams (array-like): Containing Team-IDs.\n    - data (pd.DataFrame): DataFrame with all matches in chronological order.\n    - initial_rating (float): Initial rating of an unranked team (default: 2000).\n    - k (float): K-factor, determining the impact of each match on team ratings (default: 140).\n    - alpha (float or None): Tuning parameter for the multiplier for the margin of victory. No multiplier if None.\n\n    Returns: \n    - list: Historical ratings of the winning team (WTeam).\n    - list: Historical ratings of the losing team (LTeam).\n    '''\n    \n    # Dictionary to keep track of current ratings for each team\n    team_dict = {}\n    for team in teams:\n        team_dict[team] = initial_rating\n        \n    # Lists to store ratings for each team in each game\n    r1, r2 = [], []\n    margin_of_victory = 1\n\n    # Iterate through the game data\n    for wteam, lteam, ws, ls  in tqdm(zip(data.WTeamID, data.LTeamID, data.WScore, data.LScore), total=len(data)):\n        # Append current ratings for teams to lists\n        r1.append(team_dict[wteam])\n        r2.append(team_dict[lteam])\n\n        # Calculate expected outcomes based on Elo ratings\n        rateW = 1 / (1 + 10 ** ((team_dict[lteam] - team_dict[wteam]) / initial_rating))\n        rateL = 1 / (1 + 10 ** ((team_dict[wteam] - team_dict[lteam]) / initial_rating))\n        \n        if alpha:\n            margin_of_victory = (ws - ls)/alpha\n\n        # Update ratings for winning and losing teams\n        team_dict[wteam] += k * margin_of_victory * (1 - rateW)\n        team_dict[lteam] += k * margin_of_victory * (0 - rateL)\n\n        # Ensure that ratings do not go below 1\n        if team_dict[lteam] < 1:\n            team_dict[lteam] = 1\n        \n    return r1, r2\n\ndef create_elo_data(teams, data, initial_rating=2000, k=140, alpha=None):\n    '''\n    Create a DataFrame with summary statistics of Elo ratings for teams based on historical match data.\n\n    Parameters:\n    - teams (array-like): Containing Team-IDs.\n    - data (pd.DataFrame): DataFrame with all matches in chronological order.\n    - initial_rating (float): Initial rating of an unranked team (default: 2000).\n    - k (float): K-factor, determining the impact of each match on team ratings (default: 140).\n\n    Returns: \n    - DataFrame: Summary statistics of Elo ratings for teams throughout a season.\n    '''\n    \n    r1, r2 = calculate_elo(teams, data, initial_rating, k, alpha)\n    \n    # Concatenate arrays vertically\n    seasons = np.concatenate([data.Season, data.Season])\n    days = np.concatenate([data.DayNum, data.DayNum])\n    teams = np.concatenate([data.WTeamID, data.LTeamID])\n    tourney = np.concatenate([data.tourney, data.tourney])\n    ratings = np.concatenate([r1, r2])\n    # Create a DataFrame\n    rating_df = pd.DataFrame({\n        'Season': seasons,\n        'DayNum': days,\n        'TeamID': teams,\n        'Rating': ratings,\n        'Tourney': tourney\n    })\n\n    # Sort DataFrame and remove tournament data\n    rating_df.sort_values(['TeamID', 'Season', 'DayNum'], inplace=True)\n    rating_df = rating_df[rating_df['Tourney'] == 0]\n    grouped = rating_df.groupby(['TeamID', 'Season'])\n    results = grouped['Rating'].agg(['mean', 'median', 'std', 'min', 'max', 'last'])\n    results.columns = ['Rating_Mean', 'Rating_Median', 'Rating_Std', 'Rating_Min', 'Rating_Max', 'Rating_Last']\n    results['Rating_Trend'] = grouped.apply(lambda x: linregress(range(len(x)), x['Rating']).slope, include_groups=False)\n    results.reset_index(inplace=True)\n    \n    return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_elo(gender, results, teams):\n    csv_names = ['NCAATourneyCompactResults', 'RegularSeasonCompactResults']\n    csv_names = list(map(lambda x: gender + x, csv_names))\n    csvs      = list(map(lambda x: CSV[x], csv_names))\n\n    tourneys = results.copy()\n    tourneys['tourney'] = 0\n    tourneys.loc[len(csvs[0]):, 'tourney'] = 1\n    tourneys = tourneys.sort_values(['Season', 'DayNum'])\n    \n    return create_elo_data(teams.reset_index().TeamID, tourneys).drop('Season', axis=1).groupby('TeamID').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elo_m = build_elo('M', results_m, teams_m)\nelo_w = build_elo('W', results_w, teams_w)\n\ndisplay(elo_m)\ndisplay(elo_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def winner(ids):\n    id, wId, lId = ids\n\n    return int(id == wId)\n\ndef opponent(x):\n    winInt, wId, lId = x\n    win = not winInt\n    \n    return wId if win else lId\n\ndef score_diff(x):\n    winInt, wScore, lScore = x\n    win = not winInt\n    \n    return (wScore - lScore) if win else (lScore - wScore)\n\ndef build_season_results(df):\n    season_results = df\n    season_results['TeamID'] = season_results[['WTeamID', 'LTeamID']].values.tolist()\n    season_results = season_results.explode('TeamID')\n    season_results['Win'] = season_results[['TeamID', 'WTeamID', 'LTeamID']].apply(winner, axis=1)\n    season_results['Defeat'] = season_results['Win'].apply(lambda x: 1 - x)\n    season_results['Games'] = season_results['Win'] + season_results['Defeat']\n    season_results['ScoreDiff'] = season_results[['Win', 'WScore', 'LScore']].apply(score_diff, axis=1)\n    season_results['OTeamID'] = season_results[['Win', 'WTeamID', 'LTeamID']].apply(opponent, axis=1)\n    season_results['Home'] = season_results['WLoc'].apply(lambda x: int(x[0] == 'H'))\n    season_results = season_results.drop(['Season', 'DayNum', 'WTeamID', 'WScore', 'LTeamID', 'LScore', 'WLoc', 'NumOT'], axis=1)\n    season_results = season_results.groupby(by=['TeamID', 'OTeamID']).sum()\n    season_results['WinRatio'] = season_results['Win'] / season_results['Games']\n    season_results = season_results.drop(['Win', 'Defeat'], axis=1)\n\n    return season_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"season_results_m = build_season_results(results_m)\nseason_results_w = build_season_results(results_w)\n\ndisplay(season_results_m)\ndisplay(season_results_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_rpi(results):\n    win_pct = results.copy()[['WinRatio']]\n    win_pct = win_pct.groupby('TeamID').mean()\n    win_pct['WP'] = win_pct['WinRatio'] * 100\n    win_pct = win_pct.drop('WinRatio', axis=1)\n    \n    rpi = results.copy().reset_index()\n    rpi = pd.merge(rpi, win_pct, on=['TeamID'])\n    rpi = pd.merge(rpi, win_pct, left_on=['OTeamID'], right_on=['TeamID'], suffixes=('_T', '_O'))\n    \n    wp_oo = rpi[['TeamID', 'WP_O']].groupby('TeamID').mean()\n    wp_oo = wp_oo.rename(columns={'WP_O': 'WP_OO'})\n    \n    rpi = pd.merge(rpi, wp_oo, left_on=['OTeamID'], right_on=['TeamID'])\n\n    rpi['RPI'] = (rpi['WP_T'] * 0.25) + (rpi['WP_O'] * 0.50) + (rpi['WP_OO'] * 0.25)\n    \n    return rpi[['TeamID', 'OTeamID', 'RPI']].set_index(['TeamID', 'OTeamID'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rpi_m = build_rpi(season_results_m)\nrpi_w = build_rpi(season_results_w)\n\ndisplay(rpi_m)\ndisplay(rpi_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_seeds(seed):\n    res = seed[1:]\n\n    if len(res) > 2:\n        res = res[:-1]\n\n    return int(res)\n\ndef build_seeds(gender):\n    seeds = CSV[\"{}NCAATourneySeeds\".format(gender)] \n    seeds['Seed'] = seeds['Seed'].apply(clean_seeds)\n    seeds = seeds.drop('Season', axis=1)\n    seeds = seeds.groupby(by='TeamID').mean()\n    \n    return seeds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seeds_m = build_seeds('M')\nseeds_w = build_seeds('W')\n\ndisplay(seeds_m)\ndisplay(seeds_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_rankings(gender):\n    rankings = CSV[\"{}MasseyOrdinals\".format(gender)]\n    rankings = rankings.drop(['SystemName', 'RankingDayNum'], axis=1)\n    rankings = rankings.groupby(by='TeamID').mean()\n    rankings = rankings.drop('Season', axis=1)\n\n    return rankings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rankings_m = build_rankings('M')\n\nrankings_m","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_history(season_results, seeds, teams, elo, rpi, rankings=None):\n    history = season_results.join(teams, on='TeamID').join(seeds, on='TeamID').join(elo, on='TeamID').join(rpi, on=['TeamID', 'OTeamID'])\n    history = history.reset_index()\n    history = pd.merge(history, rpi.reset_index().rename(columns={'TeamID': 'OTeamID', 'OTeamID': 'TeamID'}), on=['TeamID', 'OTeamID'], suffixes=('_T', '_O'))\n    history = pd.merge(history, seeds, left_on='OTeamID', right_on='TeamID', suffixes=('_T', '_O'))\n    history['RPIDiff'] = history['RPI_T'] - history['RPI_O']\n    history['SeedDiff'] = history['Seed_T'] - history['Seed_O']\n    history = history.drop(['RPI_T', 'RPI_O', 'Seed_T', 'Seed_O'], axis=1)\n\n    if rankings is not None:\n        history = history.join(rankings, on='TeamID')\n        history = pd.merge(history, rankings, left_on='OTeamID', right_on='TeamID', suffixes=('_T', '_O'))\n        history['RankingsDiff'] = history['OrdinalRank_T'] - history['OrdinalRank_O']\n        history = history.drop(['OrdinalRank_T', 'OrdinalRank_O'], axis=1)\n    \n    return history.set_index(['TeamID', 'OTeamID']).fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_m = build_history(season_results_m, seeds_m, teams_m, elo_m, rpi_m, rankings_m)\nhistory_w = build_history(season_results_w, seeds_w, teams_w, elo_w, rpi_w)\n\ndisplay(history_m)\ndisplay(history_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_avg(history):\n    agg = {}\n    for col in history.columns:\n        if col == 'Games' or col == 'Home':\n            agg[col] = 'sum'\n        else:\n            agg[col] = 'mean'\n    \n    avg = history.groupby('TeamID').agg(agg)\n    \n    return avg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_m = build_avg(history_m)\navg_w = build_avg(history_w)\n\ndisplay(avg_m)\ndisplay(avg_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_matchups(gender):\n    teams = CSV[\"{}Teams\".format(gender)].copy()\n    teams = teams[['TeamID']]\n    teams = pd.merge(teams, teams, how='cross')\n    teams = teams.rename(columns={'TeamID_x': 'TeamID', 'TeamID_y': 'OTeamID'})\n    teams = teams[teams['TeamID'] != teams['OTeamID']]\n    teams = teams.set_index(['TeamID', 'OTeamID'])\n\n    return teams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matchups_m = build_matchups('M')\nmatchups_w = build_matchups('W')\n\ndisplay(matchups_m)\ndisplay(matchups_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_df(history, matchups, avg):\n    df = pd.merge(matchups, history, on=['TeamID', 'OTeamID'], how='left')\n    df = df.fillna(avg).fillna(0)\n\n    if 'FirstD1Season' in df.columns:\n        df['FirstD1Season'] = df['FirstD1Season'].astype(int)\n        df['LastD1Season'] = df['LastD1Season'].astype(int)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_m = build_df(history_m, matchups_m, avg_m)\ndf_w = build_df(history_w, matchups_w, avg_w)\n\ndisplay(df_m)\ndisplay(df_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature analysis","metadata":{}},{"cell_type":"code","source":"corr_m = df_m.corr()\ncorr_m.style.background_gradient(cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_w = df_w.corr()\ncorr_w.style.background_gradient(cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_m = df_m.corr()['WinRatio'].sort_values(ascending=False)\nhigh_corr_m = corr_m[[abs(corr_m) > 0.1 for corr_m in corr_m]]\n\ncorr_w = df_w.corr()['WinRatio'].sort_values(ascending=False)\nhigh_corr_w = corr_w[[abs(corr_w) > 0.1 for corr_w in corr_w]]\n\ndisplay(high_corr_m)\ndisplay(high_corr_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def score_dataset(lgbm_params, X, y):\n    reg   = lgb.LGBMRegressor(**lgbm_params)\n    score = cross_val_score(reg, X, y)\n    score = -1 * score.mean() + score.std()\n\n    return score\n\ndef objective(trial, X, y):\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n        'num_leaves': trial.suggest_int('num_leaves', 5, 31),\n        'n_estimators': trial.suggest_int('n_estimators', 1, 100),\n        'min_child_samples': trial.suggest_int('min_child_samples', 20, 300),\n        'device_type': device(),\n        'verbose': -1\n    }\n\n    return score_dataset(params, X, y)\n\ndef study(X, y):\n    study = op.create_study()\n    study.optimize(lambda trial: objective(trial, X, y), n_trials=100, n_jobs=-1, show_progress_bar=True)\n\n    return study.best_params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_x_y(df):\n    target_column = 'WinRatio'\n    feature_columns = df.columns.tolist()\n    feature_columns.remove(target_column)\n    \n    return df[feature_columns], df[target_column]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_m, y_m = build_x_y(df_m)\nX_w, y_w = build_x_y(df_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_m = study(X_m, y_m)\nparams_w = study(X_w, y_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(X, y, params):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n    reg_test = lgb.LGBMRegressor(**params)\n    reg_test.fit(X_train, y_train)\n\n    print('LightGBM Model accuracy score: {0:0.4f}'.format(reg_test.score(X_test, y_test)))\n    print('LightGBM Model accuracy score [train]: {0:0.4f}'.format(reg_test.score(X_train, y_train)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy(X_m, y_m, params_m)\naccuracy(X_w, y_w, params_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"def build_wins(X, y, params):\n    reg = lgb.LGBMRegressor(**params)\n    reg.fit(X, y)\n\n    wins = X\n    wins['WinRatio'] = reg.predict(X)\n    wins = wins[['WinRatio']]\n\n    return wins","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wins_m = build_wins(X_m, y_m, params_m)\nwins_w = build_wins(X_w, y_w, params_w)\n\ndisplay(wins_m)\ndisplay(wins_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_slots(gender):\n    slots = CSV[\"{}NCAATourneySlots\".format(gender)]\n    year = np.max(slots['Season'])\n    \n    slots = slots[slots['Season'] == year]\n    slots = slots[slots['Slot'].str.contains('R')] \n\n    return slots","metadata":{"execution":{"iopub.status.busy":"2024-03-13T19:09:00.856974Z","iopub.execute_input":"2024-03-13T19:09:00.85757Z","iopub.status.idle":"2024-03-13T19:09:00.905153Z","shell.execute_reply.started":"2024-03-13T19:09:00.857516Z","shell.execute_reply":"2024-03-13T19:09:00.903923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slots_m = build_slots('M')\nslots_w = build_slots('W')\n\ndisplay(slots_m)\ndisplay(slots_w)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_seeds_2024():\n    seeds_2024 = CSV['2024_tourney_seeds']\n\n    return seeds_2024[seeds_2024['Tournament'] == 'M'], seeds_2024[seeds_2024['Tournament'] == 'W']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seeds_2024_m, seeds_2024_w = build_seeds_2024()\n\ndisplay(seeds_2024_m)\ndisplay(seeds_2024_w)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(seeds):\n    seed_dict = seeds.set_index('Seed')['TeamID'].to_dict()\n    inverted_seed_dict = {value: key for key, value in seed_dict.items()}\n\n    return seed_dict, inverted_seed_dict\n\n\ndef simulate(round_slots, seeds, inverted_seeds, wins):\n    '''\n    Simulates each round of the tournament.\n\n    Parameters:\n    - round_slots: DataFrame containing information on who is playing in each round.\n    - seeds (dict): Dictionary mapping seed values to team IDs.\n    - inverted_seeds (dict): Dictionary mapping team IDs to seed values.\n    - wins (DF): DF that includes wins prediction per matchup.\n    Returns:\n    - list: List with winning team IDs for each match.\n    - list: List with corresponding slot names for each match.\n    '''\n    winners = []\n    slots = []\n\n    for slot, strong, weak in zip(round_slots.Slot, round_slots.StrongSeed, round_slots.WeakSeed):\n        team_1, team_2 = seeds[strong], seeds[weak]\n\n        team_1_prob = wins.loc[team_1, team_2].WinRatio\n        winner = np.random.choice([team_1, team_2], p=[team_1_prob, 1 - team_1_prob])\n\n        # Append the winner and corresponding slot to the lists\n        winners.append(winner)\n        slots.append(slot)\n\n        seeds[slot] = winner\n\n    return [inverted_seeds[w] for w in winners], slots\n\n\ndef run_simulation(seeds, round_slots, wins, brackets):\n    '''\n    Runs a simulation of bracket tournaments.\n\n    Parameters:\n    - seeds (pd.DataFrame): DataFrame containing seed information.\n    - round_slots (pd.DataFrame): DataFrame containing information about the tournament rounds.\n    - wins (DF): DF that includes wins prediction per matchup.\n    - brackets (int): Number of brackets to simulate.\n    Returns:\n    - pd.DataFrame: DataFrame with simulation results.\n    '''\n    # Get relevant data for the simulation\n    seed_dict, inverted_seed_dict = prepare_data(seeds)\n    # Lists to store simulation results\n    results = []\n    bracket = []\n    slots = []\n\n    # Iterate through the specified number of brackets\n    for b in tqdm(range(1, brackets + 1)):\n        # Run single simulation\n        r, s = simulate(round_slots, seed_dict, inverted_seed_dict, wins)\n        \n        # Update results\n        results.extend(r)\n        bracket.extend([b] * len(r))\n        slots.extend(s)\n\n    # Create final DataFrame\n    result_df = pd.DataFrame({'Bracket': bracket, 'Slot': slots, 'Team': results})\n\n    return result_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_brackets = 100000\nresult_m = run_simulation(seeds_2024_m, slots_m, wins_m, n_brackets)\nresult_m.insert(0, 'Tournament', 'M')\nresult_w = run_simulation(seeds_2024_w, slots_w, wins_w, n_brackets)\nresult_w.insert(0, 'Tournament', 'W')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([result_m, result_w])\nsubmission.reset_index(inplace=True, drop=True)\nsubmission.index.names = ['RowId']\n\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resources\n- General guidance: https://www.kaggle.com/code/toshimelonhead/ncaa-march-madness-sabermetric-spin-v2 by _toshimelonhead_.\n- Simulation based on: https://www.kaggle.com/code/lennarthaupts/simulate-n-brackets by _Lennart Haupts_.\n- ELO rating calculation based on: https://www.kaggle.com/code/lennarthaupts/calculate-elo-ratings by _Lennart Haupts_.\n- GPU support based on: https://www.kaggle.com/code/albertespin/programmatically-check-if-gpu-is-enabled-in-kaggle by _Albert Esp√≠n_.","metadata":{}}]}